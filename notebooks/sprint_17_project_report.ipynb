{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What steps were performed and what steps were skpped? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only step I truely skipped was that I had to use read_csv() and to_csv() to load and download the DataFrames created in the script files I created for hyperparamet tuning and model testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Therefore, the code looked like this:\n",
    "```\n",
    "# Data for LGBM and XGBoost\n",
    "train_df_light_xgb = features_train.copy()  \n",
    "train_df_light_xgb['target'] = target_train  \n",
    "train_df_light_xgb.to_csv('train_data_light_xgb.csv', index=False)\n",
    "\n",
    "test_df_light_xgb = features_test.copy()  \n",
    "test_df_light_xgb['target'] = target_test  \n",
    "train_df_light_xgb.to_csv('test_data_light_xgb.csv', index=False)\n",
    "\n",
    "# Data for CatBoost\n",
    "train_df_cat = features_train.copy()  \n",
    "train_df_cat['target'] = target_train  \n",
    "train_df_cat.to_csv('train_data_cat.csv', index=False)\n",
    "\n",
    "test_df_cat = features_test.copy()  \n",
    "test_df_cat['target'] = target_test  \n",
    "test_df_cat.to_csv('test_data_cat.csv', index=False)\n",
    "```\n",
    "Instead of like this:\n",
    "```\n",
    "# Data for LGBM and XGBoost\n",
    "train_df_light_xgb = features_train.copy()  \n",
    "train_df_light_xgb['target'] = target_train  \n",
    "train_df_light_xgb.to_parquet('train_data_light_xgb.parquet', index=False)\n",
    "\n",
    "test_df_light_xgb = features_test.copy()  \n",
    "test_df_light_xgb['target'] = target_test  \n",
    "train_df_light_xgb.to_parquet('test_data_light_xgb.parquet', index=False)\n",
    "\n",
    "# Data for CatBoost\n",
    "train_df_cat = features_train.copy()  \n",
    "train_df_cat['target'] = target_train  \n",
    "train_df_cat.to_parquet('train_data_cat.parquet', index=False)\n",
    "\n",
    "test_df_cat = features_test.copy()  \n",
    "test_df_cat['target'] = target_test  \n",
    "test_df_cat.to_parquet('test_data_cat.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was simply due to the fact that pyarrow was not installed on TTT's notebook, and when I tried to run a pip install I got an error message informing me that I did not have permission to make that installation (see below) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-20.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m146.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/.venv/lib/python3.9/site-packages/pyarrow'\n",
      "Check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting shap\n",
      "  Downloading shap-0.48.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in /.venv/lib/python3.9/site-packages (from shap) (1.21.2)\n",
      "Requirement already satisfied: scipy in /.venv/lib/python3.9/site-packages (from shap) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /.venv/lib/python3.9/site-packages (from shap) (0.24.1)\n",
      "Requirement already satisfied: pandas in /.venv/lib/python3.9/site-packages (from shap) (1.2.4)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /.venv/lib/python3.9/site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in /.venv/lib/python3.9/site-packages (from shap) (25.0)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numba>=0.54 in /.venv/lib/python3.9/site-packages (from shap) (0.58.0)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions in /.venv/lib/python3.9/site-packages (from shap) (4.2.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /.venv/lib/python3.9/site-packages (from numba>=0.54->shap) (0.41.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /.venv/lib/python3.9/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /.venv/lib/python3.9/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /.venv/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.17.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /.venv/lib/python3.9/site-packages (from scikit-learn->shap) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /.venv/lib/python3.9/site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Downloading shap-0.48.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (997 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.2/997.2 kB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: slicer, cloudpickle, shap\n",
      "\u001b[?25l\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/.venv/lib/python3.9/site-packages/slicer'\n",
      "Check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [slicer]\n",
      "\u001b[1A\u001b[2KRequirement already satisfied: catboost in /.venv/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: graphviz in /.venv/lib/python3.9/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /.venv/lib/python3.9/site-packages (from catboost) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /.venv/lib/python3.9/site-packages (from catboost) (1.21.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /.venv/lib/python3.9/site-packages (from catboost) (1.2.4)\n",
      "Requirement already satisfied: scipy in /.venv/lib/python3.9/site-packages (from catboost) (1.10.1)\n",
      "Requirement already satisfied: plotly in /.venv/lib/python3.9/site-packages (from catboost) (5.4.0)\n",
      "Requirement already satisfied: six in /.venv/lib/python3.9/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /.venv/lib/python3.9/site-packages (from pandas>=0.24.0->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /.venv/lib/python3.9/site-packages (from pandas>=0.24.0->catboost) (2025.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /.venv/lib/python3.9/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /.venv/lib/python3.9/site-packages (from matplotlib->catboost) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /.venv/lib/python3.9/site-packages (from matplotlib->catboost) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /.venv/lib/python3.9/site-packages (from matplotlib->catboost) (3.2.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /.venv/lib/python3.9/site-packages (from plotly->catboost) (9.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install shap\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, I performed all of the steps that were written up in the plan. I ran the model with different sets of features and performed a SHAP analysis to discern when and where data leakage was occurring. Then I chose the set of features that lead to the best roc_auc score with no signs of data leakage. Finally, I added the threshold asdjustment to the model that had the best roc_auc score so that the accuracy score would converge on the roc_auc score.\n",
    "\n",
    "However, there are a couple of things which I did not do that I perhaps should have. Specifically, I did not calculate the recall scores nor optimal decision thresholds for the LightGBM and CatBoost models. In retrospect, this would have been worth doing because even though the XGBoost model had the best roc_auc score during cross validation, it had the worst accuracy score. Moreover, since ultiately the value of the model in this case is the ability to predict which customers are likely to churn, and secondarily which are not, some combination of accuracy and recall really is what's important here. Something I learned as a result of this is that accuracy scores will not always improve in a way that is proportional to a model's roc_auc score with a threshold adjustment. I had assumed this would be the case when I started the project, which is why I was not initially concerned with calculating an optimal threshold and recall score for each model. If I had this project to do over again, I would have calculated the recall score and optimal threshold during the cross validation phase, and selected a model accordinlgy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What difficulties did I encounter? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most difficult part of the task was finding a combination of features that optimized performance without leading to data leakage. To solve this problem I ran a SHAP analysis on each model I ran, and, based on that analysis, I tried to come up with a story in each instance of how the given set of features could contribute to data leakage, and how the given set of features could contribute to genunely improved performance.\n",
    "\n",
    "For example, while it seemed plausible that dayofweek features were not a dead giveaway to begin_date, and could therefore be paired with 'customer_duration' without leading to data leakage, there was also a really plausible story about how it could give away the begin date, and there was not a plausible story about how it could contribute so much predictive power so as to improve the roc_auc score from a .89 to a .99. Such jumps in performance, were, thusly, a major red flag, and ultimately I had to drop all dayofweek features (as well as all other date features) from the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What were the key steps to solving the task? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key steps to solving the task included engineering the 'customer_duration' and the 'p_i_or_b' feature, although the former contributed way more to the model's decision making according to the SHAP output. \n",
    "\n",
    "The 'customer_duration' feature turned out to be especially important because including all of the date features in conjunction with total_charges would have almost certainly led to data leakage due to the heavy correlation between 'total_charges' and 'customer_duration'. However, removing total_charges from that combination of features led to less than optimal performance. Therefore, creating the 'customer_duration' feature allowed us to drop all of the date features without losing performance. Moreover, we can see from the SHAP output that, although there is a heavy correlation between 'customer_durtaion' and 'total_charges', 'total_charges' still made a significant contribution to the model's prediction. \n",
    "\n",
    "Another key step to solving the task was subtracting a range of float values from 'customer_duration'. Not only did this help prevent data leakage, subtracting random float values between 12 and 15 allows the model to make predictions based on a churn date that is in the future. This is essential of Interconnect wants to be able to send out promotional offers before a churn takes place. \n",
    "\n",
    "See below for the code that was used for this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "```\n",
    "# Generate a different random value between 13 and 15 for each row\n",
    "random_values = np.random.uniform(12, 15, size=len(all_data))\n",
    "\n",
    "#Check to make sure our random values are what we expect them to be\n",
    "print(random_values.max())\n",
    "print(random_values.min())\n",
    "\n",
    "# Subtract random float values from 'customer_duration\n",
    "all_data['customer_duration'] = all_data['customer_duration'] - random_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What was the final model and what quality score did it have? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model was an XGBoost Classifier with the following hyperparameters:\n",
    "\n",
    "Best parameters found:  {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'scale_pos_weight': 3, 'subsample': 1.0}\n",
    "\n",
    "It performed with a roc_auc score of 0.85 and an accuracy score of 0.76 during cross validation. We then calculated an optimal decision threshold to be 0.4946 for positive churn classifications. \n",
    "\n",
    "When trained on the full training set with the adjusted decision threshold the model performed with a roc_auc score of 0.90, an accuracy score of 0.79 and a recall score of 0.88. Given that the goal of the task was to predict which customers will churn ahead of time so they can be offered promotional packages, this combination of accuracy and recall efficacy is suitable for the task at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment</b> This looks like a good report, nicely done!\n",
    "<a class=\"tocSkip\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
